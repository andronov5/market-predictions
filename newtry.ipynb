{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84263beb-49da-4c5e-94f4-c5bcb43d3ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5ca6ca-fee9-4c45-90e9-7cdd1cd69a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import warnings, logging\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06160b1-6281-4450-ad31-63a8ce9abfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded!\n"
     ]
    }
   ],
   "source": [
    "from model.features import (\n",
    "    download_or_load_prices,\n",
    "    compute_features,\n",
    "    data_prep_and_feature_engineering,\n",
    ")\n",
    "from model.grid_search import run_grid_search\n",
    "from model.backtest import run_backtest\n",
    "\n",
    "print(\"downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f49524-c200-4086-b97e-51d6b477fbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\features.py:27: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  raw = yf.download(\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\features.py:27: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  raw = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "market columns -> ['SPY', '^VIX']\n"
     ]
    }
   ],
   "source": [
    "start = dt.date(2015, 1, 1)\n",
    "end   = dt.date(2024, 7, 1)\n",
    "\n",
    "# --- download SPY close prices ---\n",
    "spy = download_or_load_prices(\n",
    "    [\"SPY\"],\n",
    "    Path(\"spy_cache.parquet\"),\n",
    "    start,\n",
    "    end\n",
    ")[\"Close\"].rename(\"SPY\")\n",
    "\n",
    "# --- download ^VIX close prices ---\n",
    "vix = download_or_load_prices(\n",
    "    [\"^VIX\"],\n",
    "    Path(\"vix_cache.parquet\"),\n",
    "    start,\n",
    "    end\n",
    ")[\"Close\"].rename(\"^VIX\")\n",
    "\n",
    "# --- combine into one DataFrame with plain columns ---\n",
    "market = pd.concat([spy, vix], axis=1)\n",
    "\n",
    "print(\"market columns ->\", market.columns.tolist())   # should be ['SPY', '^VIX']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8903df-e75b-4f81-9ecc-c8587c7afc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\features.py:27: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  raw = yf.download(\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\features.py:91: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df[\"VIX_Change\"] = vix.pct_change(5)\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\features.py:27: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  raw = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1860, 23) | y_train shape: (1860,)\n"
     ]
    }
   ],
   "source": [
    "# 1. raw prices for your ticker\n",
    "df_prices = download_or_load_prices(\n",
    "    [\"AAPL\"],\n",
    "    Path(\"price_cache.parquet\"),\n",
    "    start,\n",
    "    end\n",
    ")\n",
    "\n",
    "# 2. compute all features + target\n",
    "df_feat = compute_features(df_prices, market)\n",
    "\n",
    "# 3. build inputs for the next helper\n",
    "feature_list = [c for c in df_feat.columns if c != \"Target\"]  # list of feature names\n",
    "tickers      = [\"AAPL\"]                                       # list of tickers\n",
    "\n",
    "# 4. prepare training data (function returns X_train_sel, y_train)\n",
    "X_train, y_train = data_prep_and_feature_engineering(\n",
    "    tickers,\n",
    "    feature_list,\n",
    "    Path(\"feature_cache.parquet\"),\n",
    "    start,\n",
    "    end\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"| y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4596f80e-c17f-411d-b382-34718a4869d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:23,379] A new study created in memory with name: no-name-0cb3ff65-ebeb-4313-9398-481cbc189ff1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳  Running Optuna grid search (50 trials, CPU/GPU‑safe) …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.006492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:28,462] Trial 0 finished with value: 0.0 and parameters: {'n': 100, 'd': 13, 'leaf': 1, 'xgb_n': 250, 'xgb_lr': 0.015552057664651233, 'xgb_subsample': 0.9466490115962272, 'lgb_n': 150, 'lgb_lr': 0.035039058358495793, 'lgb_subsample': 0.8567860913856472}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.626756\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:29,001] Trial 1 finished with value: 0.0 and parameters: {'n': 400, 'd': 7, 'leaf': 2, 'xgb_n': 400, 'xgb_lr': 0.029116161161689278, 'xgb_subsample': 0.7945724356374081, 'lgb_n': 450, 'lgb_lr': 0.014486924367834794, 'lgb_subsample': 0.5452953513917103}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627354\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004591 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:29,469] Trial 2 finished with value: 0.0 and parameters: {'n': 500, 'd': 11, 'leaf': 3, 'xgb_n': 350, 'xgb_lr': 0.05796254078087107, 'xgb_subsample': 0.9384231335378937, 'lgb_n': 150, 'lgb_lr': 0.03974598395792699, 'lgb_subsample': 0.5502585881628353}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.627914\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:29,998] Trial 3 finished with value: 0.0 and parameters: {'n': 200, 'd': 16, 'leaf': 1, 'xgb_n': 150, 'xgb_lr': 0.05239450138539807, 'xgb_subsample': 0.728730497128243, 'lgb_n': 500, 'lgb_lr': 0.022426211195342285, 'lgb_subsample': 0.8898226597711356}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.627439\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003586 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:30,420] Trial 4 finished with value: 0.0 and parameters: {'n': 400, 'd': 10, 'leaf': 3, 'xgb_n': 350, 'xgb_lr': 0.21007479319166214, 'xgb_subsample': 0.8102941499680795, 'lgb_n': 450, 'lgb_lr': 0.04570511088425378, 'lgb_subsample': 0.8163248010369681}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627859\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003488 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:30,893] Trial 5 finished with value: 0.0 and parameters: {'n': 300, 'd': 11, 'leaf': 2, 'xgb_n': 350, 'xgb_lr': 0.015092785153556722, 'xgb_subsample': 0.868621594228231, 'lgb_n': 350, 'lgb_lr': 0.05064643510836468, 'lgb_subsample': 0.661869813289166}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627692\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003895 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:31,412] Trial 6 finished with value: 0.0 and parameters: {'n': 100, 'd': 9, 'leaf': 2, 'xgb_n': 350, 'xgb_lr': 0.03952721461564927, 'xgb_subsample': 0.843146182310046, 'lgb_n': 100, 'lgb_lr': 0.020137299238218134, 'lgb_subsample': 0.6834196105448884}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627386\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:31,827] Trial 7 finished with value: 0.0 and parameters: {'n': 400, 'd': 9, 'leaf': 4, 'xgb_n': 450, 'xgb_lr': 0.10086592037627971, 'xgb_subsample': 0.6617872420718967, 'lgb_n': 150, 'lgb_lr': 0.16379820026092248, 'lgb_subsample': 0.695463469133066}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626303\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:32,252] Trial 8 finished with value: 0.0 and parameters: {'n': 100, 'd': 19, 'leaf': 3, 'xgb_n': 400, 'xgb_lr': 0.060907181308218455, 'xgb_subsample': 0.6154227525666515, 'lgb_n': 200, 'lgb_lr': 0.15827069376217862, 'lgb_subsample': 0.617934739385889}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626263\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004379 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:32,849] Trial 9 finished with value: 0.0 and parameters: {'n': 300, 'd': 7, 'leaf': 4, 'xgb_n': 200, 'xgb_lr': 0.043419284491176435, 'xgb_subsample': 0.8666124770491477, 'lgb_n': 400, 'lgb_lr': 0.011455287258133462, 'lgb_subsample': 0.594323550347189}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627368\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003515 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:33,393] Trial 10 finished with value: 0.0 and parameters: {'n': 200, 'd': 15, 'leaf': 1, 'xgb_n': 250, 'xgb_lr': 0.010743315439584847, 'xgb_subsample': 0.9825609077614265, 'lgb_n': 250, 'lgb_lr': 0.09504543069401014, 'lgb_subsample': 0.9912422408324202}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626592\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003565 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:34,167] Trial 11 finished with value: 0.0 and parameters: {'n': 500, 'd': 5, 'leaf': 1, 'xgb_n': 500, 'xgb_lr': 0.022438547894202673, 'xgb_subsample': 0.5210514753922679, 'lgb_n': 300, 'lgb_lr': 0.011273604941481375, 'lgb_subsample': 0.805873003023282}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.627352\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:34,673] Trial 12 finished with value: 0.0 and parameters: {'n': 400, 'd': 15, 'leaf': 2, 'xgb_n': 100, 'xgb_lr': 0.02311520750441683, 'xgb_subsample': 0.7316819738202369, 'lgb_n': 300, 'lgb_lr': 0.025535187283360634, 'lgb_subsample': 0.5111884845447514}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627676\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003121 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:35,097] Trial 13 finished with value: 0.0 and parameters: {'n': 200, 'd': 13, 'leaf': 1, 'xgb_n': 250, 'xgb_lr': 0.022904148408207575, 'xgb_subsample': 0.9235671901611586, 'lgb_n': 500, 'lgb_lr': 0.016818122792331542, 'lgb_subsample': 0.916795345235229}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.627504\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:35,488] Trial 14 finished with value: 0.0 and parameters: {'n': 400, 'd': 5, 'leaf': 2, 'xgb_n': 250, 'xgb_lr': 0.010528097236438813, 'xgb_subsample': 0.7924451279322208, 'lgb_n': 400, 'lgb_lr': 0.08477338724695081, 'lgb_subsample': 0.7640094617242661}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626782\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003201 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:35,845] Trial 15 finished with value: 0.0 and parameters: {'n': 100, 'd': 13, 'leaf': 1, 'xgb_n': 450, 'xgb_lr': 0.02886418269381657, 'xgb_subsample': 0.9723764100595838, 'lgb_n': 250, 'lgb_lr': 0.031011689538341523, 'lgb_subsample': 0.8654706148798794}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.627363\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003328 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:36,233] Trial 16 finished with value: 0.0 and parameters: {'n': 300, 'd': 18, 'leaf': 2, 'xgb_n': 300, 'xgb_lr': 0.016225400766723356, 'xgb_subsample': 0.6585042435503086, 'lgb_n': 100, 'lgb_lr': 0.0691703617408428, 'lgb_subsample': 0.7414959127562002}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.626741\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:36,595] Trial 17 finished with value: 0.0 and parameters: {'n': 200, 'd': 7, 'leaf': 1, 'xgb_n': 200, 'xgb_lr': 0.1364159444693026, 'xgb_subsample': 0.9112065961381732, 'lgb_n': 400, 'lgb_lr': 0.2912946859608891, 'lgb_subsample': 0.9783302247184288}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.630194\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:37,026] Trial 18 finished with value: 0.0 and parameters: {'n': 300, 'd': 7, 'leaf': 2, 'xgb_n': 500, 'xgb_lr': 0.016284664659259776, 'xgb_subsample': 0.7865057545784006, 'lgb_n': 200, 'lgb_lr': 0.015282909223854323, 'lgb_subsample': 0.8349541391638253}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.6273\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003390 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:37,417] Trial 19 finished with value: 0.0 and parameters: {'n': 500, 'd': 17, 'leaf': 3, 'xgb_n': 300, 'xgb_lr': 0.03404541454195944, 'xgb_subsample': 0.8968780079533063, 'lgb_n': 350, 'lgb_lr': 0.032116729325033386, 'lgb_subsample': 0.7505682379595013}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.626869\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003137 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:37,881] Trial 20 finished with value: 0.0 and parameters: {'n': 400, 'd': 14, 'leaf': 1, 'xgb_n': 400, 'xgb_lr': 0.07000247897283673, 'xgb_subsample': 0.5000276874089975, 'lgb_n': 450, 'lgb_lr': 0.010020095238917101, 'lgb_subsample': 0.9423736414556425}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.627272\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:38,297] Trial 21 finished with value: 0.0 and parameters: {'n': 500, 'd': 11, 'leaf': 3, 'xgb_n': 400, 'xgb_lr': 0.0732992616117014, 'xgb_subsample': 0.9422270766052097, 'lgb_n': 150, 'lgb_lr': 0.0381351947618453, 'lgb_subsample': 0.5196612216382027}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.627929\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.002690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:38,667] Trial 22 finished with value: 0.0 and parameters: {'n': 500, 'd': 12, 'leaf': 3, 'xgb_n': 300, 'xgb_lr': 0.0295142988600322, 'xgb_subsample': 0.9845885306704584, 'lgb_n': 150, 'lgb_lr': 0.06383647592409898, 'lgb_subsample': 0.5491917239619248}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.626756\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003683 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:39,039] Trial 23 finished with value: 0.0 and parameters: {'n': 500, 'd': 9, 'leaf': 3, 'xgb_n': 350, 'xgb_lr': 0.10074670674639206, 'xgb_subsample': 0.8369191400978043, 'lgb_n': 200, 'lgb_lr': 0.02877596508796986, 'lgb_subsample': 0.5740047812511958}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.627493\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:39,414] Trial 24 finished with value: 0.0 and parameters: {'n': 400, 'd': 12, 'leaf': 4, 'xgb_n': 450, 'xgb_lr': 0.0474385870654076, 'xgb_subsample': 0.9419028492092927, 'lgb_n': 250, 'lgb_lr': 0.04078765048361123, 'lgb_subsample': 0.6235952106089226}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.627907\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.006539 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:39,903] Trial 25 finished with value: 0.0 and parameters: {'n': 500, 'd': 8, 'leaf': 2, 'xgb_n': 200, 'xgb_lr': 0.013958556178237304, 'xgb_subsample': 0.8936507650289663, 'lgb_n': 100, 'lgb_lr': 0.014435149304333714, 'lgb_subsample': 0.5457385117674739}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627358\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.002989 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:40,359] Trial 26 finished with value: 0.0 and parameters: {'n': 300, 'd': 20, 'leaf': 3, 'xgb_n': 400, 'xgb_lr': 0.018655189050833597, 'xgb_subsample': 0.7024062614363904, 'lgb_n': 150, 'lgb_lr': 0.018833964469513433, 'lgb_subsample': 0.6529065508140441}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.627406\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:40,908] Trial 27 finished with value: 0.0 and parameters: {'n': 400, 'd': 11, 'leaf': 2, 'xgb_n': 250, 'xgb_lr': 0.2748760610407723, 'xgb_subsample': 0.7676218579665748, 'lgb_n': 200, 'lgb_lr': 0.10949481958555665, 'lgb_subsample': 0.7080183128006708}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.625515\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:41,474] Trial 28 finished with value: 0.0 and parameters: {'n': 500, 'd': 14, 'leaf': 4, 'xgb_n': 350, 'xgb_lr': 0.03316122980510681, 'xgb_subsample': 0.9996710240399755, 'lgb_n': 350, 'lgb_lr': 0.061163194046273996, 'lgb_subsample': 0.578872213081034}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.626776\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003469 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:42,062] Trial 29 finished with value: 0.0 and parameters: {'n': 100, 'd': 16, 'leaf': 1, 'xgb_n': 150, 'xgb_lr': 0.05074805253475313, 'xgb_subsample': 0.824706802143846, 'lgb_n': 500, 'lgb_lr': 0.02328981537468073, 'lgb_subsample': 0.8832240710393767}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.627611\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004182 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:42,557] Trial 30 finished with value: 0.0 and parameters: {'n': 200, 'd': 10, 'leaf': 2, 'xgb_n': 300, 'xgb_lr': 0.08368818927266745, 'xgb_subsample': 0.5629565497759933, 'lgb_n': 250, 'lgb_lr': 0.034930640466508206, 'lgb_subsample': 0.7722059828591683}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.62676\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:43,090] Trial 31 finished with value: 0.0 and parameters: {'n': 200, 'd': 17, 'leaf': 1, 'xgb_n': 100, 'xgb_lr': 0.05547216725823211, 'xgb_subsample': 0.7402242964619626, 'lgb_n': 500, 'lgb_lr': 0.046199262389878745, 'lgb_subsample': 0.8914252866609047}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627842\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004174 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:32:44,747] Trial 32 finished with value: 0.0 and parameters: {'n': 100, 'd': 15, 'leaf': 1, 'xgb_n': 150, 'xgb_lr': 0.04021078862777122, 'xgb_subsample': 0.7043372371042348, 'lgb_n': 450, 'lgb_lr': 0.02105874972388042, 'lgb_subsample': 0.8266543826663918}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.627499\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:32:45,439] Trial 33 finished with value: 0.0 and parameters: {'n': 200, 'd': 10, 'leaf': 1, 'xgb_n': 150, 'xgb_lr': 0.11833867711420322, 'xgb_subsample': 0.8619165876199213, 'lgb_n': 450, 'lgb_lr': 0.025702973915571237, 'lgb_subsample': 0.9279617914823958}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.627684\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:33,798] Trial 34 finished with value: 0.0 and parameters: {'n': 100, 'd': 6, 'leaf': 2, 'xgb_n': 350, 'xgb_lr': 0.02630241670942347, 'xgb_subsample': 0.8045102894400794, 'lgb_n': 500, 'lgb_lr': 0.04708567801160186, 'lgb_subsample': 0.9544146494931296}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.101689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:39,244] Trial 35 finished with value: 0.0 and parameters: {'n': 300, 'd': 11, 'leaf': 3, 'xgb_n': 400, 'xgb_lr': 0.013133536229293271, 'xgb_subsample': 0.6438978447171516, 'lgb_n': 100, 'lgb_lr': 0.013246839827072308, 'lgb_subsample': 0.7969414980535761}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.6272\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.008592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:40,428] Trial 36 finished with value: 0.0 and parameters: {'n': 400, 'd': 16, 'leaf': 2, 'xgb_n': 350, 'xgb_lr': 0.16931292172237716, 'xgb_subsample': 0.6965728488823371, 'lgb_n': 450, 'lgb_lr': 0.01945697506289234, 'lgb_subsample': 0.8524185934857436}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627264\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004867 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:41,329] Trial 37 finished with value: 0.0 and parameters: {'n': 100, 'd': 14, 'leaf': 1, 'xgb_n': 200, 'xgb_lr': 0.05776777560884746, 'xgb_subsample': 0.9509605541721033, 'lgb_n': 350, 'lgb_lr': 0.05367562442123203, 'lgb_subsample': 0.6624539305449864}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627594\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:42,404] Trial 38 finished with value: 0.0 and parameters: {'n': 300, 'd': 8, 'leaf': 3, 'xgb_n': 450, 'xgb_lr': 0.03760190829195214, 'xgb_subsample': 0.6124548512433708, 'lgb_n': 150, 'lgb_lr': 0.03925674430803585, 'lgb_subsample': 0.6288717378024272}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.627918\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004579 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:43,211] Trial 39 finished with value: 0.0 and parameters: {'n': 200, 'd': 9, 'leaf': 4, 'xgb_n': 250, 'xgb_lr': 0.018800024700643854, 'xgb_subsample': 0.874033372162455, 'lgb_n': 400, 'lgb_lr': 0.02604201344211222, 'lgb_subsample': 0.5507101353802587}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.627662\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.005017 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:43,984] Trial 40 finished with value: 0.0 and parameters: {'n': 400, 'd': 18, 'leaf': 1, 'xgb_n': 300, 'xgb_lr': 0.0670799440984764, 'xgb_subsample': 0.7592059971401761, 'lgb_n': 300, 'lgb_lr': 0.017216040269772876, 'lgb_subsample': 0.7248607790235004}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.627482\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:44,468] Trial 41 finished with value: 0.0 and parameters: {'n': 400, 'd': 10, 'leaf': 3, 'xgb_n': 350, 'xgb_lr': 0.19358335477754707, 'xgb_subsample': 0.8172741768476233, 'lgb_n': 450, 'lgb_lr': 0.04499186180962312, 'lgb_subsample': 0.9018789910011602}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627884\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.002757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:33:45,115] Trial 42 finished with value: 0.0 and parameters: {'n': 400, 'd': 12, 'leaf': 3, 'xgb_n': 400, 'xgb_lr': 0.29513405371716306, 'xgb_subsample': 0.8537887125809245, 'lgb_n': 500, 'lgb_lr': 0.022306793380173936, 'lgb_subsample': 0.8059648776234795}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.627443\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.005095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:45,859] Trial 43 finished with value: 0.0 and parameters: {'n': 300, 'd': 13, 'leaf': 3, 'xgb_n': 350, 'xgb_lr': 0.2237040510073733, 'xgb_subsample': 0.7730832654174734, 'lgb_n': 400, 'lgb_lr': 0.07896467881679968, 'lgb_subsample': 0.8599087362240152}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.626804\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.005439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:46,358] Trial 44 finished with value: 0.0 and parameters: {'n': 200, 'd': 6, 'leaf': 3, 'xgb_n': 100, 'xgb_lr': 0.09093216598522373, 'xgb_subsample': 0.7401209311700144, 'lgb_n': 450, 'lgb_lr': 0.030195214397102103, 'lgb_subsample': 0.5059575223723495}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.627409\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.002820 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:33:46,827] Trial 45 finished with value: 0.0 and parameters: {'n': 500, 'd': 8, 'leaf': 2, 'xgb_n': 450, 'xgb_lr': 0.01176319116727975, 'xgb_subsample': 0.7203205735334298, 'lgb_n': 500, 'lgb_lr': 0.12247801045740893, 'lgb_subsample': 0.7829947686124156}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626272\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.006082 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n",
      "[I 2025-07-30 11:33:47,555] Trial 46 finished with value: 0.0 and parameters: {'n': 100, 'd': 9, 'leaf': 1, 'xgb_n': 300, 'xgb_lr': 0.14872730669841502, 'xgb_subsample': 0.8789662442795435, 'lgb_n': 300, 'lgb_lr': 0.012639161032698106, 'lgb_subsample': 0.8301863200218035}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627251\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:48,131] Trial 47 finished with value: 0.0 and parameters: {'n': 300, 'd': 10, 'leaf': 2, 'xgb_n': 150, 'xgb_lr': 0.019687539157751153, 'xgb_subsample': 0.6816490781085687, 'lgb_n': 200, 'lgb_lr': 0.036077469147116195, 'lgb_subsample': 0.6036014640514744}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.627955\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.002916 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:48,544] Trial 48 finished with value: 0.0 and parameters: {'n': 200, 'd': 6, 'leaf': 1, 'xgb_n': 250, 'xgb_lr': 0.025402604310727137, 'xgb_subsample': 0.9111310598466196, 'lgb_n': 400, 'lgb_lr': 0.055258044273942736, 'lgb_subsample': 0.8784706770913338}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:19: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"n\", 100, 500, 100),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:29: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"xgb_n\", 100, 500, 50),\n",
      "C:\\Users\\17012\\Desktop\\market-predictions\\model\\grid_search.py:46: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "Positional arguments ['self', 'name', 'low', 'high', 'step', 'log'] in suggest_int() have been deprecated since v3.5.0. They will be replaced with the corresponding keyword arguments in v5.0.0, so please use the keyword specification instead. See https://github.com/optuna/optuna/releases/tag/v3.5.0 for details.\n",
      "  n_estimators=trial.suggest_int(\"lgb_n\", 100, 500, 50),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:33:49,159] Trial 49 finished with value: 0.0 and parameters: {'n': 300, 'd': 11, 'leaf': 4, 'xgb_n': 400, 'xgb_lr': 0.0482631987588909, 'xgb_subsample': 0.9580694608905744, 'lgb_n': 100, 'lgb_lr': 0.016788474547408668, 'lgb_subsample': 0.9098162180152225}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.627506\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "\n",
      "===== GridSearch Results (sorted by precision) =====\n",
      " value  params_d  params_leaf  params_lgb_lr  params_lgb_n  params_lgb_subsample  params_n  params_xgb_lr  params_xgb_n  params_xgb_subsample\n",
      "   0.0        13            1       0.035039           150              0.856786       100       0.015552           250              0.946649\n",
      "   0.0        14            1       0.053676           350              0.662454       100       0.057768           200              0.950961\n",
      "   0.0        11            2       0.109495           200              0.708018       400       0.274876           250              0.767622\n",
      "   0.0        14            4       0.061163           350              0.578872       500       0.033161           350              0.999671\n",
      "   0.0        16            1       0.023290           500              0.883224       100       0.050748           150              0.824707\n",
      "   0.0        10            2       0.034931           250              0.772206       200       0.083688           300              0.562957\n",
      "   0.0        17            1       0.046199           500              0.891425       200       0.055472           100              0.740224\n",
      "   0.0        15            1       0.021059           450              0.826654       100       0.040211           150              0.704337\n",
      "   0.0        10            1       0.025703           450              0.927962       200       0.118339           150              0.861917\n",
      "   0.0         6            2       0.047086           500              0.954415       100       0.026302           350              0.804510\n",
      "   0.0        11            3       0.013247           100              0.796941       300       0.013134           400              0.643898\n",
      "   0.0        16            2       0.019457           450              0.852419       400       0.169313           350              0.696573\n",
      "   0.0         8            3       0.039257           150              0.628872       300       0.037602           450              0.612455\n",
      "   0.0         7            2       0.014487           450              0.545295       400       0.029116           400              0.794572\n",
      "   0.0         9            4       0.026042           400              0.550710       200       0.018800           250              0.874033\n",
      "   0.0        18            1       0.017216           300              0.724861       400       0.067080           300              0.759206\n",
      "   0.0        10            3       0.044992           450              0.901879       400       0.193583           350              0.817274\n",
      "   0.0        12            3       0.022307           500              0.805965       400       0.295134           400              0.853789\n",
      "   0.0        13            3       0.078965           400              0.859909       300       0.223704           350              0.773083\n",
      "   0.0         6            3       0.030195           450              0.505958       200       0.090932           100              0.740121\n",
      "   0.0         8            2       0.122478           500              0.782995       500       0.011763           450              0.720321\n",
      "   0.0         9            1       0.012639           300              0.830186       100       0.148727           300              0.878966\n",
      "   0.0        10            2       0.036077           200              0.603601       300       0.019688           150              0.681649\n",
      "   0.0         6            1       0.055258           400              0.878471       200       0.025403           250              0.911131\n",
      "   0.0        20            3       0.018834           150              0.652907       300       0.018655           400              0.702406\n",
      "   0.0         8            2       0.014435           100              0.545739       500       0.013959           200              0.893651\n",
      "   0.0        12            4       0.040788           250              0.623595       400       0.047439           450              0.941903\n",
      "   0.0         9            3       0.028776           200              0.574005       500       0.100747           350              0.836919\n",
      "   0.0        11            3       0.039746           150              0.550259       500       0.057963           350              0.938423\n",
      "   0.0        16            1       0.022426           500              0.889823       200       0.052395           150              0.728730\n",
      "   0.0        10            3       0.045705           450              0.816325       400       0.210075           350              0.810294\n",
      "   0.0        11            2       0.050646           350              0.661870       300       0.015093           350              0.868622\n",
      "   0.0         9            2       0.020137           100              0.683420       100       0.039527           350              0.843146\n",
      "   0.0         9            4       0.163798           150              0.695463       400       0.100866           450              0.661787\n",
      "   0.0        19            3       0.158271           200              0.617935       100       0.060907           400              0.615423\n",
      "   0.0         7            4       0.011455           400              0.594324       300       0.043419           200              0.866612\n",
      "   0.0        15            1       0.095045           250              0.991242       200       0.010743           250              0.982561\n",
      "   0.0         5            1       0.011274           300              0.805873       500       0.022439           500              0.521051\n",
      "   0.0        15            2       0.025535           300              0.511188       400       0.023115           100              0.731682\n",
      "   0.0        13            1       0.016818           500              0.916795       200       0.022904           250              0.923567\n",
      "   0.0         5            2       0.084773           400              0.764009       400       0.010528           250              0.792445\n",
      "   0.0        13            1       0.031012           250              0.865471       100       0.028864           450              0.972376\n",
      "   0.0        18            2       0.069170           100              0.741496       300       0.016225           300              0.658504\n",
      "   0.0         7            1       0.291295           400              0.978330       200       0.136416           200              0.911207\n",
      "   0.0         7            2       0.015283           200              0.834954       300       0.016285           500              0.786506\n",
      "   0.0        17            3       0.032117           350              0.750568       500       0.034045           300              0.896878\n",
      "   0.0        14            1       0.010020           450              0.942374       400       0.070002           400              0.500028\n",
      "   0.0        11            3       0.038135           150              0.519661       500       0.073299           400              0.942227\n",
      "   0.0        12            3       0.063836           150              0.549192       500       0.029514           300              0.984589\n",
      "   0.0        11            4       0.016788           100              0.909816       300       0.048263           400              0.958069\n",
      "\n",
      "Grid search completed. Results saved to grid_search_results.parquet\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# hyper‑parameter search\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m best_params, best_model = run_grid_search(X_train, y_train)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbest params:\u001b[39m\u001b[33m\"\u001b[39m, best_params)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# simple back‑test (use your own test split or walk‑forward logic if you prefer)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# hyper‑parameter search\n",
    "best_params, best_model = run_grid_search(X_train, y_train)\n",
    "print(\"best params:\", best_params)\n",
    "\n",
    "# simple back‑test (use your own test split or walk‑forward logic if you prefer)\n",
    "results = run_backtest(best_model, X_train, y_train)\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31db956-e964-4586-b5f5-20d20717419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e06049-4c3c-4da1-a189-7c086395d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666828d8-f908-42db-ae37-07938a7d97c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e33f7b-6846-45ac-86e7-50f94a6fe654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:16,260] A new study created in memory with name: no-name-6b297d80-5c41-4e48-875c-5f67079651d2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳  Running Optuna grid search (50 trials, CPU/GPU‑safe) …\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004716 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:16,682] Trial 0 finished with value: 0.0 and parameters: {'n': 400, 'd': 19, 'leaf': 3, 'xgb_n': 450, 'xgb_lr': 0.08674838178422838, 'xgb_subsample': 0.7056704450279433, 'lgb_n': 300, 'lgb_lr': 0.0863393630435047, 'lgb_subsample': 0.6328920505875433}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626751\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003479 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:17,106] Trial 1 finished with value: 0.0 and parameters: {'n': 300, 'd': 13, 'leaf': 4, 'xgb_n': 350, 'xgb_lr': 0.011014719334948854, 'xgb_subsample': 0.553222574462068, 'lgb_n': 400, 'lgb_lr': 0.020903032512830804, 'lgb_subsample': 0.5911701254330661}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.627505\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:17,507] Trial 2 finished with value: 0.0 and parameters: {'n': 300, 'd': 20, 'leaf': 1, 'xgb_n': 350, 'xgb_lr': 0.1243638258505135, 'xgb_subsample': 0.9446103814606868, 'lgb_n': 200, 'lgb_lr': 0.01545614464697805, 'lgb_subsample': 0.8514402296170136}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.62729\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003708 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:17,911] Trial 3 finished with value: 0.0 and parameters: {'n': 400, 'd': 17, 'leaf': 4, 'xgb_n': 400, 'xgb_lr': 0.04115995823582435, 'xgb_subsample': 0.8055655161502531, 'lgb_n': 500, 'lgb_lr': 0.023911437696612324, 'lgb_subsample': 0.5398554013525894}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627472\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003493 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:18,339] Trial 4 finished with value: 0.0 and parameters: {'n': 400, 'd': 16, 'leaf': 2, 'xgb_n': 200, 'xgb_lr': 0.07264506968277878, 'xgb_subsample': 0.9520956645298342, 'lgb_n': 400, 'lgb_lr': 0.015273548939041435, 'lgb_subsample': 0.5550573604376146}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627301\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:18,781] Trial 5 finished with value: 0.0 and parameters: {'n': 400, 'd': 13, 'leaf': 1, 'xgb_n': 300, 'xgb_lr': 0.05758522940722071, 'xgb_subsample': 0.6800937345607884, 'lgb_n': 350, 'lgb_lr': 0.019608988890201157, 'lgb_subsample': 0.9945266507620114}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627375\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:19,130] Trial 6 finished with value: 0.0 and parameters: {'n': 500, 'd': 8, 'leaf': 2, 'xgb_n': 400, 'xgb_lr': 0.01786026550430891, 'xgb_subsample': 0.7397972847582828, 'lgb_n': 100, 'lgb_lr': 0.04281119675388106, 'lgb_subsample': 0.5291954094750893}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627961\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.002800 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:19,460] Trial 7 finished with value: 0.0 and parameters: {'n': 400, 'd': 20, 'leaf': 1, 'xgb_n': 250, 'xgb_lr': 0.17164768678374842, 'xgb_subsample': 0.8034675153149075, 'lgb_n': 200, 'lgb_lr': 0.02850000625226208, 'lgb_subsample': 0.9396347697918881}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.62751\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:19,794] Trial 8 finished with value: 0.0 and parameters: {'n': 400, 'd': 13, 'leaf': 3, 'xgb_n': 100, 'xgb_lr': 0.12998591086075645, 'xgb_subsample': 0.6124179856510698, 'lgb_n': 100, 'lgb_lr': 0.1406557308287678, 'lgb_subsample': 0.544700673096449}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.62621\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003101 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:20,155] Trial 9 finished with value: 0.0 and parameters: {'n': 100, 'd': 13, 'leaf': 1, 'xgb_n': 500, 'xgb_lr': 0.26102366691940376, 'xgb_subsample': 0.9414171851131797, 'lgb_n': 150, 'lgb_lr': 0.06037110967653841, 'lgb_subsample': 0.9940153104424467}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.626784\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003083 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:20,532] Trial 10 finished with value: 0.0 and parameters: {'n': 200, 'd': 5, 'leaf': 3, 'xgb_n': 500, 'xgb_lr': 0.027888674458631732, 'xgb_subsample': 0.5149108080464699, 'lgb_n': 300, 'lgb_lr': 0.2637437797410064, 'lgb_subsample': 0.6855145346531757}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.628876\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:20,917] Trial 11 finished with value: 0.0 and parameters: {'n': 200, 'd': 9, 'leaf': 4, 'xgb_n': 450, 'xgb_lr': 0.010840997090978093, 'xgb_subsample': 0.5563737316406177, 'lgb_n': 450, 'lgb_lr': 0.08312273911219763, 'lgb_subsample': 0.6967008063095936}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626817\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.005391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:21,520] Trial 12 finished with value: 0.0 and parameters: {'n': 300, 'd': 16, 'leaf': 4, 'xgb_n': 350, 'xgb_lr': 0.011402859022654538, 'xgb_subsample': 0.6354014463269039, 'lgb_n': 300, 'lgb_lr': 0.1116914133477332, 'lgb_subsample': 0.6417457442932922}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.626063\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003839 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:22,202] Trial 13 finished with value: 0.0 and parameters: {'n': 500, 'd': 10, 'leaf': 3, 'xgb_n': 400, 'xgb_lr': 0.0859102780566515, 'xgb_subsample': 0.7116215352208158, 'lgb_n': 350, 'lgb_lr': 0.010726772710092732, 'lgb_subsample': 0.6318412548270668}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_logloss: 0.627369\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004437 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:22,700] Trial 14 finished with value: 0.0 and parameters: {'n': 200, 'd': 18, 'leaf': 4, 'xgb_n': 200, 'xgb_lr': 0.035361456772401864, 'xgb_subsample': 0.5949067991688454, 'lgb_n': 250, 'lgb_lr': 0.0420358845566551, 'lgb_subsample': 0.7923769971347684}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627989\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004312 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:23,260] Trial 15 finished with value: 0.0 and parameters: {'n': 300, 'd': 11, 'leaf': 3, 'xgb_n': 450, 'xgb_lr': 0.02092431868169791, 'xgb_subsample': 0.8495274900652309, 'lgb_n': 400, 'lgb_lr': 0.18059549204269779, 'lgb_subsample': 0.6166970502725181}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.62649\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004362 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:23,793] Trial 16 finished with value: 0.0 and parameters: {'n': 500, 'd': 15, 'leaf': 2, 'xgb_n': 300, 'xgb_lr': 0.09147875665283049, 'xgb_subsample': 0.5146745325007686, 'lgb_n': 500, 'lgb_lr': 0.0779841961942166, 'lgb_subsample': 0.7646242135587955}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.626793\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003842 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:24,375] Trial 17 finished with value: 0.0 and parameters: {'n': 100, 'd': 7, 'leaf': 4, 'xgb_n': 450, 'xgb_lr': 0.2987898650760643, 'xgb_subsample': 0.6643543643407107, 'lgb_n': 400, 'lgb_lr': 0.03706490766569837, 'lgb_subsample': 0.601190089707675}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.627941\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004390 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:24,952] Trial 18 finished with value: 0.0 and parameters: {'n': 300, 'd': 18, 'leaf': 3, 'xgb_n': 350, 'xgb_lr': 0.049590019332401584, 'xgb_subsample': 0.5727788278442211, 'lgb_n': 250, 'lgb_lr': 0.06247233579036486, 'lgb_subsample': 0.6992053601612372}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.626765\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:25,510] Trial 19 finished with value: 0.0 and parameters: {'n': 200, 'd': 11, 'leaf': 4, 'xgb_n': 100, 'xgb_lr': 0.017705375436599186, 'xgb_subsample': 0.7839293534609291, 'lgb_n': 350, 'lgb_lr': 0.10399816170316954, 'lgb_subsample': 0.836749787552362}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626458\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:26,015] Trial 20 finished with value: 0.0 and parameters: {'n': 400, 'd': 15, 'leaf': 3, 'xgb_n': 250, 'xgb_lr': 0.02844554791436774, 'xgb_subsample': 0.7101035219488762, 'lgb_n': 450, 'lgb_lr': 0.2083127914163157, 'lgb_subsample': 0.5014099783541113}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627017\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:26,664] Trial 21 finished with value: 0.0 and parameters: {'n': 300, 'd': 20, 'leaf': 2, 'xgb_n': 350, 'xgb_lr': 0.1339046433905331, 'xgb_subsample': 0.8866963609102888, 'lgb_n': 200, 'lgb_lr': 0.012278903327459805, 'lgb_subsample': 0.8705501035717845}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627284\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004295 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:27,267] Trial 22 finished with value: 0.0 and parameters: {'n': 300, 'd': 19, 'leaf': 2, 'xgb_n': 350, 'xgb_lr': 0.18862462101126345, 'xgb_subsample': 0.986563570319891, 'lgb_n': 250, 'lgb_lr': 0.015531308679624125, 'lgb_subsample': 0.8822258323908404}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627285\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004123 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:27,804] Trial 23 finished with value: 0.0 and parameters: {'n': 300, 'd': 20, 'leaf': 1, 'xgb_n': 400, 'xgb_lr': 0.10862363540326023, 'xgb_subsample': 0.8622227190079769, 'lgb_n': 200, 'lgb_lr': 0.03073963841981691, 'lgb_subsample': 0.7437764155451304}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.627378\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:28,424] Trial 24 finished with value: 0.0 and parameters: {'n': 200, 'd': 18, 'leaf': 3, 'xgb_n': 250, 'xgb_lr': 0.06166680445749297, 'xgb_subsample': 0.9103768203671252, 'lgb_n': 150, 'lgb_lr': 0.01439517499221181, 'lgb_subsample': 0.5912005451827647}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627361\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003867 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:29,019] Trial 25 finished with value: 0.0 and parameters: {'n': 300, 'd': 15, 'leaf': 2, 'xgb_n': 300, 'xgb_lr': 0.1871972859707443, 'xgb_subsample': 0.7753176278433705, 'lgb_n': 300, 'lgb_lr': 0.022445713018907028, 'lgb_subsample': 0.6625486466812703}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.627438\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004367 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:29,678] Trial 26 finished with value: 0.0 and parameters: {'n': 400, 'd': 19, 'leaf': 4, 'xgb_n': 450, 'xgb_lr': 0.11475251003691099, 'xgb_subsample': 0.6680493317548182, 'lgb_n': 300, 'lgb_lr': 0.019134596111257535, 'lgb_subsample': 0.7346621304574541}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.627394\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003814 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:30,353] Trial 27 finished with value: 0.0 and parameters: {'n': 300, 'd': 17, 'leaf': 1, 'xgb_n': 500, 'xgb_lr': 0.14869253758592293, 'xgb_subsample': 0.8301719752884739, 'lgb_n': 150, 'lgb_lr': 0.010344413435165881, 'lgb_subsample': 0.8037462449799653}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's binary_logloss: 0.627271\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004205 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:30,904] Trial 28 finished with value: 0.0 and parameters: {'n': 500, 'd': 14, 'leaf': 3, 'xgb_n': 350, 'xgb_lr': 0.07955317168086512, 'xgb_subsample': 0.7414423690697665, 'lgb_n': 450, 'lgb_lr': 0.052877125935584066, 'lgb_subsample': 0.584210223777876}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627619\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004198 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:31,481] Trial 29 finished with value: 0.0 and parameters: {'n': 400, 'd': 17, 'leaf': 4, 'xgb_n': 400, 'xgb_lr': 0.0402623102184606, 'xgb_subsample': 0.560274333838151, 'lgb_n': 350, 'lgb_lr': 0.02725009734231628, 'lgb_subsample': 0.9146885132648932}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.627586\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004696 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:32,041] Trial 30 finished with value: 0.0 and parameters: {'n': 400, 'd': 6, 'leaf': 4, 'xgb_n': 300, 'xgb_lr': 0.05082611106847819, 'xgb_subsample': 0.6248209266011133, 'lgb_n': 250, 'lgb_lr': 0.017778349644067288, 'lgb_subsample': 0.8250809637258549}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.627452\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.005500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:32,714] Trial 31 finished with value: 0.0 and parameters: {'n': 400, 'd': 17, 'leaf': 4, 'xgb_n': 400, 'xgb_lr': 0.06595841635674513, 'xgb_subsample': 0.9912351821083896, 'lgb_n': 450, 'lgb_lr': 0.01466366024784326, 'lgb_subsample': 0.5682308319465162}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627342\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004685 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:33,314] Trial 32 finished with value: 0.0 and parameters: {'n': 300, 'd': 19, 'leaf': 4, 'xgb_n': 450, 'xgb_lr': 0.10151313347309911, 'xgb_subsample': 0.9198388634747661, 'lgb_n': 500, 'lgb_lr': 0.02244153849113041, 'lgb_subsample': 0.5015141801138304}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.627438\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.005886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:33,963] Trial 33 finished with value: 0.0 and parameters: {'n': 500, 'd': 16, 'leaf': 3, 'xgb_n': 400, 'xgb_lr': 0.014099817328323496, 'xgb_subsample': 0.8241425923767179, 'lgb_n': 500, 'lgb_lr': 0.02270353979006536, 'lgb_subsample': 0.5484096852844998}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.62743\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.005503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:34,621] Trial 34 finished with value: 0.0 and parameters: {'n': 400, 'd': 19, 'leaf': 1, 'xgb_n': 350, 'xgb_lr': 0.06936849727280263, 'xgb_subsample': 0.700661049442877, 'lgb_n': 400, 'lgb_lr': 0.033350571330856636, 'lgb_subsample': 0.6526551072474561}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.626818\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:35,192] Trial 35 finished with value: 0.0 and parameters: {'n': 400, 'd': 20, 'leaf': 2, 'xgb_n': 200, 'xgb_lr': 0.041127555170798774, 'xgb_subsample': 0.9566981120205937, 'lgb_n': 350, 'lgb_lr': 0.04549891766064314, 'lgb_subsample': 0.5259013055395849}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.627866\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004674 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:35,845] Trial 36 finished with value: 0.0 and parameters: {'n': 400, 'd': 12, 'leaf': 4, 'xgb_n': 400, 'xgb_lr': 0.02331816631818698, 'xgb_subsample': 0.7546177621839897, 'lgb_n': 400, 'lgb_lr': 0.0130090884621971, 'lgb_subsample': 0.5683981455783129}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627219\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004336 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:36,458] Trial 37 finished with value: 0.0 and parameters: {'n': 300, 'd': 14, 'leaf': 3, 'xgb_n': 300, 'xgb_lr': 0.2151923143338987, 'xgb_subsample': 0.8745461189564049, 'lgb_n': 100, 'lgb_lr': 0.027473311038557387, 'lgb_subsample': 0.5292476007086084}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.627572\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004064 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:36,979] Trial 38 finished with value: 0.0 and parameters: {'n': 500, 'd': 16, 'leaf': 1, 'xgb_n': 500, 'xgb_lr': 0.014272066104760051, 'xgb_subsample': 0.8015599994815553, 'lgb_n': 500, 'lgb_lr': 0.07621014410549846, 'lgb_subsample': 0.6155206816184096}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.626775\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:37,621] Trial 39 finished with value: 0.0 and parameters: {'n': 400, 'd': 18, 'leaf': 4, 'xgb_n': 450, 'xgb_lr': 0.14568017674074055, 'xgb_subsample': 0.9003800085935643, 'lgb_n': 200, 'lgb_lr': 0.01771053968742662, 'lgb_subsample': 0.970179434113232}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.627456\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:38,232] Trial 40 finished with value: 0.0 and parameters: {'n': 200, 'd': 12, 'leaf': 3, 'xgb_n': 400, 'xgb_lr': 0.23039020506127414, 'xgb_subsample': 0.5000554971785864, 'lgb_n': 450, 'lgb_lr': 0.025208428874657934, 'lgb_subsample': 0.6722478980463767}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627661\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004340 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:38,829] Trial 41 finished with value: 0.0 and parameters: {'n': 400, 'd': 14, 'leaf': 2, 'xgb_n': 150, 'xgb_lr': 0.07786791536085567, 'xgb_subsample': 0.9232067696733977, 'lgb_n': 400, 'lgb_lr': 0.016616623357935654, 'lgb_subsample': 0.5684236309461445}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627514\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004601 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:39,495] Trial 42 finished with value: 0.0 and parameters: {'n': 300, 'd': 17, 'leaf': 1, 'xgb_n': 150, 'xgb_lr': 0.05211097830307277, 'xgb_subsample': 0.9656974780794485, 'lgb_n': 300, 'lgb_lr': 0.012904928329264723, 'lgb_subsample': 0.5444369415853921}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627228\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004249 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:40,058] Trial 43 finished with value: 0.0 and parameters: {'n': 400, 'd': 20, 'leaf': 2, 'xgb_n': 250, 'xgb_lr': 0.0940373061051719, 'xgb_subsample': 0.9489405702329763, 'lgb_n': 350, 'lgb_lr': 0.0211632700683076, 'lgb_subsample': 0.6293534565261397}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's binary_logloss: 0.627368\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:40,589] Trial 44 finished with value: 0.0 and parameters: {'n': 500, 'd': 16, 'leaf': 1, 'xgb_n': 300, 'xgb_lr': 0.1257302631621595, 'xgb_subsample': 0.5842865275389675, 'lgb_n': 450, 'lgb_lr': 0.1069696307908993, 'lgb_subsample': 0.7066056935391091}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.625396\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004341 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:41,290] Trial 45 finished with value: 0.0 and parameters: {'n': 400, 'd': 9, 'leaf': 2, 'xgb_n': 350, 'xgb_lr': 0.07155372535407796, 'xgb_subsample': 0.6441466913995292, 'lgb_n': 350, 'lgb_lr': 0.011795893603210997, 'lgb_subsample': 0.6059123370567612}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's binary_logloss: 0.627332\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004223 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:41,847] Trial 46 finished with value: 0.0 and parameters: {'n': 300, 'd': 13, 'leaf': 4, 'xgb_n': 200, 'xgb_lr': 0.03288575641591978, 'xgb_subsample': 0.8476563472136373, 'lgb_n': 400, 'lgb_lr': 0.06497330038708457, 'lgb_subsample': 0.5540915399189835}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.62675\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.005447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:42,428] Trial 47 finished with value: 0.0 and parameters: {'n': 100, 'd': 18, 'leaf': 3, 'xgb_n': 350, 'xgb_lr': 0.04273100636604332, 'xgb_subsample': 0.5368603200134112, 'lgb_n': 500, 'lgb_lr': 0.04067489470240067, 'lgb_subsample': 0.5233356125950008}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.627908\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.003610 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:43,018] Trial 48 finished with value: 0.0 and parameters: {'n': 200, 'd': 19, 'leaf': 2, 'xgb_n': 500, 'xgb_lr': 0.056473588971029186, 'xgb_subsample': 0.6882781596030829, 'lgb_n': 250, 'lgb_lr': 0.092762076593503, 'lgb_subsample': 0.7721328631080803}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626631\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "[LightGBM] [Info] Number of positive: 512, number of negative: 976\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 5865\n",
      "[LightGBM] [Info] Number of data points in the train set: 1488, number of used features: 23\n",
      "[LightGBM] [Info] Using GPU Device: gfx902, Vendor: Advanced Micro Devices, Inc.\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 23 dense feature groups (0.03 MB) transferred to GPU in 0.004288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.344086 -> initscore=-0.645138\n",
      "[LightGBM] [Info] Start training from score -0.645138\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-30 11:56:43,653] Trial 49 finished with value: 0.0 and parameters: {'n': 300, 'd': 15, 'leaf': 4, 'xgb_n': 450, 'xgb_lr': 0.010708772110867206, 'xgb_subsample': 0.6119949529991547, 'lgb_n': 150, 'lgb_lr': 0.16387973078776785, 'lgb_subsample': 0.7178529404801477}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.626303\n",
      "Trial failed due to The estimator XGBClassifier should be a classifier.; returning 0\n",
      "\n",
      "===== GridSearch Results (sorted by precision) =====\n",
      " value  params_d  params_leaf  params_lgb_lr  params_lgb_n  params_lgb_subsample  params_n  params_xgb_lr  params_xgb_n  params_xgb_subsample\n",
      "   0.0        19            3       0.086339           300              0.632892       400       0.086748           450              0.705670\n",
      "   0.0        14            3       0.027473           100              0.529248       300       0.215192           300              0.874546\n",
      "   0.0        17            1       0.010344           150              0.803746       300       0.148693           500              0.830172\n",
      "   0.0        14            3       0.052877           450              0.584210       500       0.079553           350              0.741442\n",
      "   0.0        17            4       0.027250           350              0.914689       400       0.040262           400              0.560274\n",
      "   0.0         6            4       0.017778           250              0.825081       400       0.050826           300              0.624821\n",
      "   0.0        17            4       0.014664           450              0.568231       400       0.065958           400              0.991235\n",
      "   0.0        19            4       0.022442           500              0.501514       300       0.101513           450              0.919839\n",
      "   0.0        16            3       0.022704           500              0.548410       500       0.014100           400              0.824143\n",
      "   0.0        19            1       0.033351           400              0.652655       400       0.069368           350              0.700661\n",
      "   0.0        20            2       0.045499           350              0.525901       400       0.041128           200              0.956698\n",
      "   0.0        12            4       0.013009           400              0.568398       400       0.023318           400              0.754618\n",
      "   0.0        16            1       0.076210           500              0.615521       500       0.014272           500              0.801560\n",
      "   0.0        13            4       0.020903           400              0.591170       300       0.011015           350              0.553223\n",
      "   0.0        18            4       0.017711           200              0.970179       400       0.145680           450              0.900380\n",
      "   0.0        12            3       0.025208           450              0.672248       200       0.230390           400              0.500055\n",
      "   0.0        14            2       0.016617           400              0.568424       400       0.077868           150              0.923207\n",
      "   0.0        17            1       0.012905           300              0.544437       300       0.052111           150              0.965697\n",
      "   0.0        20            2       0.021163           350              0.629353       400       0.094037           250              0.948941\n",
      "   0.0        16            1       0.106970           450              0.706606       500       0.125730           300              0.584287\n",
      "   0.0         9            2       0.011796           350              0.605912       400       0.071554           350              0.644147\n",
      "   0.0        13            4       0.064973           400              0.554092       300       0.032886           200              0.847656\n",
      "   0.0        18            3       0.040675           500              0.523336       100       0.042731           350              0.536860\n",
      "   0.0        19            2       0.092762           250              0.772133       200       0.056474           500              0.688278\n",
      "   0.0        19            4       0.019135           300              0.734662       400       0.114753           450              0.668049\n",
      "   0.0        15            2       0.022446           300              0.662549       300       0.187197           300              0.775318\n",
      "   0.0        18            3       0.014395           150              0.591201       200       0.061667           250              0.910377\n",
      "   0.0        20            1       0.030740           200              0.743776       300       0.108624           400              0.862223\n",
      "   0.0        20            1       0.015456           200              0.851440       300       0.124364           350              0.944610\n",
      "   0.0        17            4       0.023911           500              0.539855       400       0.041160           400              0.805566\n",
      "   0.0        16            2       0.015274           400              0.555057       400       0.072645           200              0.952096\n",
      "   0.0        13            1       0.019609           350              0.994527       400       0.057585           300              0.680094\n",
      "   0.0         8            2       0.042811           100              0.529195       500       0.017860           400              0.739797\n",
      "   0.0        20            1       0.028500           200              0.939635       400       0.171648           250              0.803468\n",
      "   0.0        13            3       0.140656           100              0.544701       400       0.129986           100              0.612418\n",
      "   0.0        13            1       0.060371           150              0.994015       100       0.261024           500              0.941417\n",
      "   0.0         5            3       0.263744           300              0.685515       200       0.027889           500              0.514911\n",
      "   0.0         9            4       0.083123           450              0.696701       200       0.010841           450              0.556374\n",
      "   0.0        16            4       0.111691           300              0.641746       300       0.011403           350              0.635401\n",
      "   0.0        10            3       0.010727           350              0.631841       500       0.085910           400              0.711622\n",
      "   0.0        18            4       0.042036           250              0.792377       200       0.035361           200              0.594907\n",
      "   0.0        11            3       0.180595           400              0.616697       300       0.020924           450              0.849527\n",
      "   0.0        15            2       0.077984           500              0.764624       500       0.091479           300              0.514675\n",
      "   0.0         7            4       0.037065           400              0.601190       100       0.298790           450              0.664354\n",
      "   0.0        18            3       0.062472           250              0.699205       300       0.049590           350              0.572779\n",
      "   0.0        11            4       0.103998           350              0.836750       200       0.017705           100              0.783929\n",
      "   0.0        15            3       0.208313           450              0.501410       400       0.028446           250              0.710104\n",
      "   0.0        20            2       0.012279           200              0.870550       300       0.133905           350              0.886696\n",
      "   0.0        19            2       0.015531           250              0.882226       300       0.188625           350              0.986564\n",
      "   0.0        15            4       0.163880           150              0.717853       300       0.010709           450              0.611995\n",
      "\n",
      "Grid search completed. Results saved to grid_search_results.parquet\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m; warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, category=\u001b[38;5;167;01mUserWarning\u001b[39;00m); warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, category=\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1) run the quiet grid‑search you already have\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m best_params, best_model = run_grid_search(X_train, y_train)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ best params:\u001b[39m\u001b[33m\"\u001b[39m, best_params)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 2) fit the RandomForest with only the RF‑compatible params\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# ── add these two lines FIRST ───────────────────────────────────────────────\n",
    "import joblib\n",
    "import warnings; warnings.filterwarnings(\"ignore\", category=UserWarning); warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1) run the quiet grid‑search you already have\n",
    "best_params, best_model = run_grid_search(X_train, y_train)\n",
    "print(\"✅ best params:\", best_params)\n",
    "\n",
    "# 2) fit the RandomForest with only the RF‑compatible params\n",
    "rf_params = {k: v for k, v in best_params.items()\n",
    "             if k in {\"n_estimators\", \"max_depth\", \"max_features\",\n",
    "                      \"min_samples_split\", \"min_samples_leaf\"}}\n",
    "\n",
    "best_model = RandomForestClassifier(**rf_params,\n",
    "                                   n_jobs=-1,\n",
    "                                   random_state=42).fit(X_train, y_train)\n",
    "print(\"🔥  model fitted\")\n",
    "\n",
    "# 3) build bt_data exactly as before  (df_feat, idx_dates, proba …)\n",
    "# … your existing code that constructs `bt_data` …\n",
    "\n",
    "# 4) save artefact the way backtest.py expects\n",
    "art_path = Path(\"best_model.joblib\")\n",
    "joblib.dump({\"model\": best_model,\n",
    "             \"features\": X_train.columns.tolist(),\n",
    "             \"bt_data\": bt_data},\n",
    "            art_path)\n",
    "print(\"📦 artefact saved to\", art_path)\n",
    "\n",
    "# 5) run the back‑test (only a Path arg now)\n",
    "results = run_backtest(art_path)\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a68cc-c6d1-472f-9e5e-2ec1d395001b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
