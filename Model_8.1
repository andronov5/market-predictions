# OPTIMIZED KERNEL 1 — Data Prep + Feature Selection + **Grid Search ONLY**
"""
1. **Removed downstream model fit / threshold tuning / artefact save.**  The script stops right
   after Optuna finishes and prints a sortable DataFrame of all trials.
2. Keeps feature engineering + selection so the grid search is meaningful.
3. Writes optional grid_search_results.parquet for later inspection.

Run‑time flow
─────────────
1. Download / cache OHLCV.
2. Parallel feature engineering.
3. RandomForest feature selection (≤80 % cumulative importances).  Selected features printed.
4. **Optuna 10‑trial Bayesian search** on RandomForest+XGB+LightGBM ensemble.
5. Prints a table of param sets & precision scores sorted descending.

You can now eyeball the hyper‑param landscape before deciding which model to train.
"""

import argparse
import datetime as dt
import json
import os
from pathlib import Path
from typing import List

import backoff
import gspread
from google.auth import default as gauth_default
import joblib
import lightgbm as lgbm
import matplotlib.pyplot as plt
import numpy as np
import optuna
import pandas as pd
import ta
import vectorbt as vbt
import xgboost as xgb
import yfinance as yf
from joblib import Parallel, delayed
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import precision_score
from ta.volatility import BollingerBands

# ───────────── CONFIG ─────────────
CACHE_FILE = Path("prices_5y.parquet")
TICKERS = [
    "AAPL","MSFT","TSLA","GOOGL","AMZN","NVDA","META","NFLX","AMD","BABA",
    "V","JPM","BAC","KO","DIS","XOM","CVX","INTC","IBM","ORCL"
]
YEARS = 5
END_DATE = dt.date.today()
START_DATE = END_DATE - dt.timedelta(days=YEARS*365)

FEATURES = [
    "RSI","MACD","MACD_SIGNAL","SMA_10","SMA_50","EMA_10","EMA_50","SMA_ratio","MFI","ATR",
    "BOLL_HBAND","BOLL_LBAND","BOLL_WIDTH","Return_1d","Return_5d","Return_10d","Return_20d",
    "Volatility_10d","Volatility_20d","SPY_Trend","VIX_Level","VIX_Change","RS_Market"
]
N_JOBS = -1

# ──────────── ARGPARSE ────────────
def parse_args():
    """Parse CLI arguments."""
    parser = argparse.ArgumentParser(description="Run Model 8.1 pipeline steps")
    parser.add_argument(
        "--tickers",
        type=str,
        default=",".join(TICKERS),
        help="Comma separated list of tickers",
    )
    parser.add_argument(
        "--start",
        type=lambda s: dt.datetime.strptime(s, "%Y-%m-%d").date(),
        default=START_DATE,
        help="Start date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--end",
        type=lambda s: dt.datetime.strptime(s, "%Y-%m-%d").date(),
        default=END_DATE,
        help="End date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--grid-search",
        action="store_true",
        help="Run the Optuna grid search step",
    )
    parser.add_argument(
        "--backtest",
        action="store_true",
        help="Run the vectorbt backtest step",
    )
    parser.add_argument(
        "--update-sheets",
        action="store_true",
        help="Update Google Sheets with portfolio values",
    )
    parser.add_argument(
        "--n-trials",
        type=int,
        default=50,
        help="Number of Optuna trials for grid search",
    )
    parser.add_argument(
        "--feat-threshold",
        type=float,
        default=0.8,
        help="Cumulative importance threshold for feature selection",
    )
    parser.add_argument(
        "--lookahead-days",
        type=int,
        default=5,
        help="Days to look ahead when generating the Target column",
    )
    parser.add_argument(
        "--target-pct",
        type=float,
        default=0.02,
        help="Percent rise threshold for classifying Target=1",
    )
    return parser.parse_args()

# ───────── data cache helper ─────────

def download_or_load_prices(tickers):
    def _download(start, end):
        raw = yf.download(" ".join(tickers), start=start, end=end, progress=False, group_by="ticker")
        return raw.stack(level=0, future_stack=True).swaplevel().sort_index()

    if CACHE_FILE.exists():
        try:
            df = pd.read_parquet(CACHE_FILE)
            lvl0 = pd.to_datetime(df.index.get_level_values(0), errors="coerce")
            if lvl0.isna().any():
                raise ValueError("corrupt index detected")
            last_cached = lvl0.max().date()
            if last_cached < END_DATE - dt.timedelta(days=1):
                df = pd.concat([df, _download(last_cached + dt.timedelta(days=1), END_DATE)])
        except Exception:
            CACHE_FILE.unlink(missing_ok=True)
            df = _download(START_DATE, END_DATE)
    else:
        df = _download(START_DATE, END_DATE)
    df.to_parquet(CACHE_FILE)
    return df

# ───────── feature engineering ─────────

def compute_features(
    df,
    market,
    lookahead_days: int = 5,
    target_pct: float = 0.02,
):
    df = df.dropna(subset=["Close", "High", "Low", "Open", "Volume"]).copy()
    df["Target"] = (
        df["Close"].shift(-lookahead_days) >= df["Close"] * (1 + target_pct)
    ).astype(int)
    df["RSI"] = ta.momentum.rsi(df["Close"], 14)
    df["MACD"] = ta.trend.macd(df["Close"], 26, 12)
    df["MACD_SIGNAL"] = ta.trend.macd_signal(df["Close"], 26, 12, 9)
    df["SMA_10"] = ta.trend.sma_indicator(df["Close"], 10)
    df["SMA_50"] = ta.trend.sma_indicator(df["Close"], 50)
    df["EMA_10"] = ta.trend.ema_indicator(df["Close"], 10)
    df["EMA_50"] = ta.trend.ema_indicator(df["Close"], 50)
    df["SMA_ratio"] = df["SMA_10"] / df["SMA_50"]
    df["MFI"] = ta.volume.money_flow_index(df["High"], df["Low"], df["Close"], df["Volume"], 14)
    df["ATR"] = ta.volatility.average_true_range(df["High"], df["Low"], df["Close"], 14)
    bb = BollingerBands(df["Close"], 20, 2)
    df["BOLL_HBAND"] = bb.bollinger_hband()
    df["BOLL_LBAND"] = bb.bollinger_lband()
    df["BOLL_WIDTH"] = (df["BOLL_HBAND"] - df["BOLL_LBAND"]) / df["Close"]
    for w in (1,5,10,20):
        df[f"Return_{w}d"] = df["Close"].pct_change(w)
    df["Volatility_10d"] = df["Close"].pct_change().rolling(10).std()
    df["Volatility_20d"] = df["Close"].pct_change().rolling(20).std()
    spy = market["SPY"].reindex(df.index).ffill()
    vix = market["^VIX"].reindex(df.index).ffill()
    df["SPY_Trend"] = (spy > spy.rolling(20).mean()).astype(int)
    df["VIX_Level"] = vix
    df["VIX_Change"] = vix.pct_change(5)
    df["RS_Market"] = (df["Close"] / df["Close"].shift(20)) / (spy / spy.shift(20))
    return df.dropna()

def data_prep_and_feature_engineering(
    threshold: float = 0.8,
    lookahead_days: int = 5,
    target_pct: float = 0.02,
):
    """Download prices, compute features and return selected training data."""
    ALL_TICKERS = TICKERS + ["SPY", "^VIX"]
    prices = download_or_load_prices(ALL_TICKERS)

    price_dict = {
        tkr: grp.droplevel("Ticker")
        for tkr, grp in prices.groupby(level="Ticker")
        if tkr in ALL_TICKERS
    }
    market_dict = {k: price_dict[k]["Close"] for k in ["SPY", "^VIX"]}

    print("⚙️  Computing features …")
    feat_list = Parallel(n_jobs=N_JOBS)(
        delayed(compute_features)(
            price_dict[tkr], market_dict, lookahead_days, target_pct
        )
        for tkr in TICKERS
    )
    all_data = pd.concat(feat_list, keys=TICKERS)
    all_data.index.names = ["Date", "Symbol"]

    # ───────── split train / test ─────────
    cut = int(len(all_data) * 0.85)
    train_df = all_data.iloc[:cut]
    X_train, y_train = train_df[FEATURES], train_df["Target"]

    # ───────── feature selection ─────────
    print("🔍  Feature selection via RandomForest …")
    rf_sel = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42).fit(
        X_train, y_train
    )
    imp = pd.Series(rf_sel.feature_importances_, index=FEATURES).sort_values(
        ascending=False
    )
    selected_features = imp.loc[(imp.cumsum() / imp.sum()) <= threshold].index.tolist()
    print(f"Selected {len(selected_features)} features → {selected_features}\n")

    X_train_sel = X_train[selected_features]
    return X_train_sel, y_train

# ───────── Bayesian GRID SEARCH (Optuna) ─────────

def run_grid_search(X_train_sel, y_train, n_trials: int = 50):
    """Run Optuna Bayesian search and print results."""

    def objective(trial):
        # 80/20 time‑based split for early stopping
        split = int(len(X_train_sel) * 0.8)
        X_tr, X_val = X_train_sel.iloc[:split], X_train_sel.iloc[split:]
        y_tr, y_val = y_train.iloc[:split], y_train.iloc[split:]

        rf = RandomForestClassifier(
            n_estimators=trial.suggest_int("n", 100, 500, 100),
            max_depth=trial.suggest_int("d", 5, 20),
            min_samples_leaf=trial.suggest_int("leaf", 1, 4),
            class_weight="balanced",
            n_jobs=-1,
            random_state=42,
        )

        # ------- XGB CPU/GPU fallback -------
        try:
            xgbc = xgb.XGBClassifier(
                tree_method="gpu_hist",
                gpu_id=0,
                max_depth=6,
                n_estimators=trial.suggest_int("xgb_n", 100, 500, 50),
                learning_rate=trial.suggest_float("xgb_lr", 0.01, 0.3, log=True),
                subsample=trial.suggest_float("xgb_subsample", 0.5, 1.0),
                random_state=42,
                eval_metric="logloss",
            )
        except xgb.core.XGBoostError:
            xgbc = xgb.XGBClassifier(
                tree_method="hist",
                max_depth=6,
                n_estimators=trial.suggest_int("xgb_n", 100, 500, 50),
                learning_rate=trial.suggest_float("xgb_lr", 0.01, 0.3, log=True),
                subsample=trial.suggest_float("xgb_subsample", 0.5, 1.0),
                random_state=42,
                eval_metric="logloss",
            )

        xgbc.fit(
            X_tr,
            y_tr,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=20,
            verbose=False,
        )

        # ------- LightGBM CPU/GPU fallback -------
        try:
            lgbc = lgbm.LGBMClassifier(
                device_type="gpu",
                n_estimators=trial.suggest_int("lgb_n", 100, 500, 50),
                learning_rate=trial.suggest_float("lgb_lr", 0.01, 0.3, log=True),
                subsample=trial.suggest_float("lgb_subsample", 0.5, 1.0),
                random_state=42,
            )
        except Exception:
            lgbc = lgbm.LGBMClassifier(
                device_type="cpu",
                n_estimators=trial.suggest_int("lgb_n", 100, 500, 50),
                learning_rate=trial.suggest_float("lgb_lr", 0.01, 0.3, log=True),
                subsample=trial.suggest_float("lgb_subsample", 0.5, 1.0),
                random_state=42,
            )

        lgbc.fit(
            X_tr,
            y_tr,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=20,
            verbose=False,
        )

        vote = VotingClassifier([("rf", rf), ("xgb", xgbc), ("lgb", lgbc)], voting="soft", n_jobs=-1)
        try:
            vote.fit(X_tr, y_tr)
            pred = vote.predict(X_val)
            score = precision_score(y_val, pred, pos_label=1, zero_division=0)
        except Exception as e:
            print(f"Trial failed due to {e}; returning 0")
            score = 0.0
        return score

    print(f"⏳  Running Optuna grid search ({n_trials} trials, CPU/GPU‑safe) …")
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=n_trials, timeout=600)

    results = study.trials_dataframe().sort_values("value", ascending=False)
    print("\n===== GridSearch Results (sorted by precision) =====")
    print(
        results[["value"] + [c for c in results.columns if c.startswith("params_")]].to_string(index=False)
    )

    results.to_parquet("grid_search_results.parquet", index=False)
    print("\nGrid search completed. Results saved to grid_search_results.parquet")

# OPTIMIZED KERNEL 2  — Vectorised, GPU‑friendly Backtest & Equity Tracker
"""
Ultrafast replacement for the original Backtrader notebook.
Runs a daily‑bar multi‑asset backtest in **vectorbt** (<1 s for 5 years × 20 tickers)
with stop‑loss, trailing‑stop and take‑profit baked in.  Outputs:
- cumulative equity curve
- per‑trade stats, Sharpe, max drawdown
- JSON list of daily portfolio values (for the Sheets uploader)
- PNG plot of equity & drawdowns

**Prereqs**
bash
pip install vectorbt pandas numpy matplotlib joblib plotly kaleido

Vectorbt uses Numba under‑the‑hood; if you have a CUDA GPU and numba‑cuda, it will JIT on GPU.
"""

ARTEFACT_FILE = Path("ml_pipeline.joblib")  # created by kernel 1
INIT_CASH = 100_000
MAX_POS_PCT = 0.10          # 10 % of NAV per trade
STOP_LOSS_PCT = 0.05        # hard stop
TRAIL_PCT = 0.03            # trailing stop (only when in profit)
TAKE_PROFIT_PCT = 0.08      # target
MAX_CONCURRENT = 5


def run_backtest():
    """Run the vectorbt backtest and export results."""
    print("📂  Loading artefacts …")
    art = joblib.load(ARTEFACT_FILE)
    df_bt = art["bt_data"]

    df_bt["Date"] = pd.to_datetime(df_bt["Date"])
    df_bt.set_index("Date", inplace=True)

    price_close = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Close").ffill()
    entries_raw = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Predicted").fillna(0).astype(bool)
    probs = df_bt.pivot_table(index=df_bt.index, columns="Symbol", values="Probability").fillna(0.0)

    conf = ((probs - 0.5).clip(lower=0) / 0.5).rolling(1).mean()
    size_pct = MAX_POS_PCT * (0.5 + 0.5 * conf)

    exit_after = 10
    exit_timeout = entries_raw.shift(exit_after).fillna(False)

    exits_sl, exits_tp = vbt.tsignals.generate_sl_tp(
        price_close, entries_raw, stop_loss=STOP_LOSS_PCT, take_profit=TAKE_PROFIT_PCT
    )
    exits_trail = vbt.tsignals.generate_trailing(price_close, entries_raw, trail_percent=TRAIL_PCT)
    exits_combined = exits_sl | exits_tp | exits_trail | exit_timeout

    print("🚀  Running vectorbt portfolio …")
    pf = vbt.Portfolio.from_signals(
        price_close,
        entries_raw,
        exits_combined,
        size=size_pct * INIT_CASH,
        init_cash=INIT_CASH,
        freq="1D",
        max_orders=MAX_CONCURRENT,
    )

    print("✅  Done.   Final NAV = ${:,.2f}".format(pf.final_value()))
    print("Sharpe Ratio : {:.2f}".format(pf.sharpe_ratio()))
    print("Max Drawdown : {:.2%}".format(pf.max_drawdown()))
    print("Total Trades : {}".format(pf.trades.count()))
    print("Win Rate     : {:.2%}".format(pf.trades.win_rate()))

    port_vals = pf.value()
    with open("portfolio_values.json", "w") as f:
        json.dump(port_vals.round(2).to_list(), f)

    plt.figure(figsize=(12, 7))
    plt.subplot(2, 1, 1)
    plt.plot(port_vals.index, port_vals.values, label="Equity")
    plt.title("Equity Curve")
    plt.grid(True)
    plt.legend()

    plt.subplot(2, 1, 2)
    max_curve = port_vals.cummax()
    plt.fill_between(port_vals.index, 0, (port_vals / max_curve - 1).values, color="red", alpha=0.3)
    plt.title("Drawdown")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("backtest_results.png", dpi=150)
    plt.close()

    print("📊  Outputs: portfolio_values.json  +  backtest_results.png")

# OPTIMIZED KERNEL 3  — Trading Log / Equity Tracker ⟷ Google Sheets
"""
Lean, incremental Google Sheets updater that appends new equity values (or trade rows)
without rewriting the entire worksheet.  Designed to be called **after**
portfolio_values.json is produced by Kernel 2.

Key upgrades vs. original:
──────────────────────────
1. **Incremental writes** – detect last row, append only new data.
2. **Batch mode** – groups rows in ≤500‑row chunks → 50‑100× faster than update().
3. **Single credential load** – service‑account creds cached in a global.
4. **Exponential back‑off** – auto‑retries on transient HTTP 429/503 errors.
5. **Environment variables** – no hard‑coded paths; set GOOGLE_APPLICATION_CREDENTIALS.
6. **Generic append()** – works for both the *Equity Tracker* and *Trades* sheets.

Install:
bash
pip install gspread google-auth google-auth-httplib2 backoff

• Ensure the service account e‑mail has edit rights to the target sheet.
• Set env var GOOGLE_APPLICATION_CREDENTIALS=/full/path/creds.json.
"""

SPREADSHEET_NAME = "TradingLog"        # Google Sheets file name
EQUITY_SHEET = "Equity Tracker"        # tab name for equity curve
TRADES_SHEET = "Trades"                # optional tab for trade history
PORTFOLIO_FILE = Path("portfolio_values.json")
# ─────────────────────── GSPREAD CLIENT ──────────────────────

def get_gs_client():
    """Reuse an authenticated gspread client (service account or application‑default)."""
    creds, _ = gauth_default(scopes=[
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ])
    return gspread.authorize(creds)


def get_sheet():
    """Return the Google Sheet without opening it at import time."""
    client = get_gs_client()
    return client.open(SPREADSHEET_NAME)

# ─────────────────────── HELPERS ─────────────────────────────

def _ensure_worksheet(sheet, title: str, cols: int = 3):
    try:
        return sheet.worksheet(title)
    except gspread.WorksheetNotFound:
        return sheet.add_worksheet(title, rows="100", cols=str(cols))


def _last_row(ws):
    """Return index (1‑based) of last non‑empty row, 0 if fresh."""
    str_vals = ws.col_values(1)
    return len(str_vals)


@backoff.on_exception(backoff.expo, (gspread.exceptions.APIError,), max_tries=5)
def _append_rows(ws, rows: List[list]):
    """Batch‑append rows to worksheet with retry back‑off."""
    # gspread's append_rows uses Sheets API's batchUpdate; this is fast.
    ws.append_rows(rows, value_input_option="USER_ENTERED")

# ─────────────────────── EQUITY TRACKER ─────────────────────

def update_equity_tracker(json_path: Path = PORTFOLIO_FILE):
    """Append equity data from ``json_path`` to the Equity Tracker sheet.

    ``json_path`` should point to a JSON file containing the list of
    daily portfolio equity values.
    """
    if not json_path.exists():
        raise FileNotFoundError(json_path)

    with open(json_path) as f:
        values = json.load(f)  # list[float]

    sheet = get_sheet()
    ws = _ensure_worksheet(sheet, EQUITY_SHEET)
    start_idx = _last_row(ws)

    header = ["Date", "Equity", "Daily PnL", "Cum PnL"]
    if start_idx == 0:
        _append_rows(ws, [header])
        start_idx = 1

    # build rows to append
    rows = []
    init_equity = values[0]
    prev_equity = init_equity if start_idx <= 1 else float(ws.cell(start_idx, 2).value)

    for i, val in enumerate(values[start_idx - 1 :], start=start_idx - 1):
        date = (dt.date.today() - dt.timedelta(days=len(values) - 1 - i)).isoformat()
        daily_pnl = val - prev_equity
        cum_pnl = val - init_equity
        rows.append([date, f"{val:.2f}", f"{daily_pnl:.2f}", f"{cum_pnl:.2f}"])
        prev_equity = val

    if rows:
        # split into ≤500‑row batches (API limit guard)
        for j in range(0, len(rows), 500):
            _append_rows(ws, rows[j : j + 500])
        print(f"✔ Added {len(rows)} equity rows (through {rows[-1][0]}).")
    else:
        print("ℹ Equity sheet already up‑to‑date.")

# ───────────────────── MAIN (CLI) ────────────────────────────
def main():
    args = parse_args()

    global TICKERS, START_DATE, END_DATE, YEARS
    TICKERS = [t.strip() for t in args.tickers.split(',') if t.strip()]
    START_DATE = args.start
    END_DATE = args.end
    YEARS = (END_DATE - START_DATE).days // 365

    run_all = not (args.grid_search or args.backtest or args.update_sheets)

    if args.grid_search or run_all:
        X_train_sel, y_train = data_prep_and_feature_engineering(
            args.feat_threshold,
            args.lookahead_days,
            args.target_pct,
        )
        run_grid_search(X_train_sel, y_train, args.n_trials)

    if args.backtest or run_all:
        run_backtest()

    if args.update_sheets or run_all:
        update_equity_tracker()


if __name__ == "__main__":
    main()
    # To log trades, write a similar function append_trades(trade_df)
